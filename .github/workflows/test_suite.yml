name: AI Assistant Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run validation tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements_test.txt

    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: codecov-unit-tests

  validation-tests:
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements_test.txt

    - name: Run validation tests
      run: |
        python tests/validate_model_performance.py --blackjack-only
        python tests/validate_model_performance.py --poker-only

    - name: Upload validation results
      uses: actions/upload-artifact@v3
      with:
        name: validation-results
        path: tests/results/

  performance-tests:
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements_test.txt

    - name: Run performance tests
      run: |
        python tests/validate_model_performance.py --performance-only

  integration-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, validation-tests]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements_test.txt

    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ -v --tb=short

  continuous-validation:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: 3.9

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements_test.txt

    - name: Run continuous validation
      run: |
        timeout 300 python tests/validate_model_performance.py --continuous --interval 5

  test-summary:
    runs-on: ubuntu-latest
    needs: [unit-tests, validation-tests, integration-tests, performance-tests]
    if: always()

    steps:
    - name: Test Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Type | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY

        # Check job statuses and add to summary
        if [ "${{ needs.unit-tests.result }}" == "success" ]; then
          echo "| Unit Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Unit Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.validation-tests.result }}" == "success" ]; then
          echo "| Validation Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Validation Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.integration-tests.result }}" == "success" ]; then
          echo "| Integration Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Integration Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.performance-tests.result }}" == "success" ]; then
          echo "| Performance Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Performance Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📊 **Test execution completed at:** $(date)" >> $GITHUB_STEP_SUMMARY